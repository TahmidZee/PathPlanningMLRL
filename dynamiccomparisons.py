# -*- coding: utf-8 -*-
"""DynamicComparisons.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P0VGoOofRiq-CJq8Vc0SglQq0KDjdehc
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib notebook


import gym
from gym import spaces
import numpy as np
import random
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.animation import FuncAnimation

from IPython.display import HTML

class AdvancedRobotEnv(gym.Env):
    """A more complex environment for robotic navigation with static and dynamic obstacles."""

    def __init__(self):
        super(AdvancedRobotEnv, self).__init__()
        self.grid_size = 20  # Define the grid size
        self.action_space = spaces.Discrete(4)  # Define the action space (up, down, left, right)
        self.num_static_obstacles = 3  # Define the number of static obstacles
        self.num_dynamic_obstacles = 2  # Define the number of dynamic obstacles
        self.observation_space = spaces.Box(low=0, high=self.grid_size,
                                            shape=(2 + 2 * self.num_static_obstacles + 2 * self.num_dynamic_obstacles,), dtype=np.int32)

        self.state = None  # Initialize the state
        self.goal_position = np.array([self.grid_size - 1, self.grid_size - 1])  # Set the goal position
        self.ani = None  # To keep the reference to the animation

        # Generate static and dynamic obstacles
        self.static_obstacles = [self._random_position() for _ in range(self.num_static_obstacles)]
        self.dynamic_obstacles = [self._random_position() for _ in range(self.num_dynamic_obstacles)]
        self.reset()

    def reset(self):
        """Reset the environment state."""
        self.state = np.array([0, 0])  # Starting position of the robot
        return self.state

    def step(self, action):
        """Update the environment state based on an action."""
        # Move the robot
        self.state[:2] = self._move_robot(self.state[:2], action)

        # Update dynamic obstacles
        for i in range(self.num_dynamic_obstacles):
            self.dynamic_obstacles[i] = self._move_obstacle(self.dynamic_obstacles[i])

        # Check for collisions with static and dynamic obstacles
        collision_static = any(np.array_equal(self.state[:2], obstacle) for obstacle in self.static_obstacles)
        collision_dynamic = any(np.array_equal(self.state[:2], obstacle) for obstacle in self.dynamic_obstacles)
        collision = collision_static or collision_dynamic

        # Determine if the goal has been reached
        done = np.array_equal(self.state[:2], self.goal_position)
        reward = self._calculate_reward(done, collision)

        return self.state, reward, done, {}

    def _random_position(self):
        """Generate a random position within the grid."""
        return np.array([random.randint(0, self.grid_size - 1), random.randint(0, self.grid_size - 1)])

    def _move_robot(self, position, action):
        """Move the robot based on the action."""
        moves = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # up, down, left, right
        new_position = position + moves[action]
        new_position = np.clip(new_position, 0, self.grid_size - 1)
        return new_position.astype(int)

    def _move_obstacle(self, position):
        """Randomly move a dynamic obstacle to a new position."""
        moves = [(-1, 0), (1, 0), (0, -1), (0, 1), (0, 0)]  # Add a stationary move
        move = random.choice(moves)
        new_position = position + move
        new_position = np.clip(new_position, 0, self.grid_size - 1)
        return new_position.astype(int)

    def _check_collision(self):
        """Check if the robot has collided with any obstacle."""
        for obstacle in self.static_obstacles + self.dynamic_obstacles:
            if np.array_equal(self.state[:2], obstacle):
                return True
        return False

    def _calculate_reward(self, done, collision):
        """Calculate the reward based on the current state."""
        if done:
            return 10  # High reward for reaching the goal
        elif collision:
            return -10  # High penalty for collision
        else:
            return -1  # Small penalty for each step to encourage efficiency

    def animate_step(self, i, fig, ax):
        """Animation step function."""
        ax.clear()
        ax.set_xlim(0, self.grid_size)
        ax.set_ylim(0, self.grid_size)
        plt.grid()

        # Update robot and obstacles positions
        action = self.action_space.sample()  # Replace with your algorithm's action
        self.step(action)

        # Draw the robot
        robot = patches.Circle((self.state[1] + 0.5, self.grid_size - self.state[0] - 0.5), 0.4, color='blue')
        ax.add_patch(robot)

        # Draw static obstacles
        for obstacle in self.static_obstacles:
            static = patches.Rectangle((obstacle[1] + 0.1, self.grid_size - obstacle[0] - 0.9), 0.8, 0.8, color='grey')
            ax.add_patch(static)

        # Draw dynamic obstacles
        for obstacle in self.dynamic_obstacles:
            dynamic = patches.Rectangle((obstacle[1] + 0.1, self.grid_size - obstacle[0] - 0.9), 0.8, 0.8, color='orange')
            ax.add_patch(dynamic)

        # Draw the goal
        goal = patches.Rectangle((self.goal_position[1] + 0.1, self.grid_size - self.goal_position[0] - 0.9), 0.8, 0.8, color='green')
        ax.add_patch(goal)

    def start_animation(self, steps=50):
        """Start the environment animation."""
        fig, ax = plt.subplots()
        ani = FuncAnimation(fig, self.animate_step, frames=steps, fargs=(fig, ax), interval=200)
        return HTML(ani.to_jshtml())

# Testing the environment with animation
env = AdvancedRobotEnv()
env.reset()
env.start_animation(steps=50)  # Start the animation

class QLearningAgent:
    def __init__(self, learning_rate=0.1, discount_factor=0.9, epsilon=0.1, action_size=4):
        self.q_table = {}
        self.lr = learning_rate
        self.gamma = discount_factor
        self.epsilon = epsilon
        self.action_size = action_size

    def choose_action(self, state):
        if np.random.rand() < self.epsilon:
            return np.random.randint(self.action_size)  # Explore
        return np.argmax(self.q_table.get(state, np.zeros(self.action_size)))  # Exploit

    def learn(self, state, action, reward, next_state):
        old_value = self.q_table.get(state, np.zeros(self.action_size))[action]
        next_max = np.max(self.q_table.get(next_state, np.zeros(self.action_size)))

        new_value = (1 - self.lr) * old_value + self.lr * (reward + self.gamma * next_max)
        if state not in self.q_table:
            self.q_table[state] = np.zeros(self.action_size)
        self.q_table[state][action] = new_value

def train(env, agent, episodes):
    for episode in range(episodes):
        state = env.reset()
        done = False

        while not done:
            action = agent.choose_action(str(state))
            next_state, reward, done, _ = env.step(action)
            agent.learn(str(state), action, reward, str(next_state))
            state = next_state

# Initialize environment and agent
env = AdvancedRobotEnv()
agent = QLearningAgent()

# Train the agent
train(env, agent, episodes=1000)



def heuristic(a, b):
    return abs(a[0] - b[0]) + abs(a[1] - b[1])

def a_star_search(env, start, goal):
    frontier = PriorityQueue()
    frontier.put(start, 0)
    came_from = {}
    cost_so_far = {}
    came_from[start] = None
    cost_so_far[start] = 0

    while not frontier.empty():
        current = frontier.get()

        if current == goal:
            break

        for next in env.get_neighbors(current):
            if next in env.static_obstacles or next in env.dynamic_obstacles:
                continue  # Skip nodes that are obstacles

            new_cost = cost_so_far[current] + 1  # Assuming a grid, cost between neighbors is 1
            if next not in cost_so_far or new_cost < cost_so_far[next]:
                cost_so_far[next] = new_cost
                priority = new_cost + heuristic(goal, next)
                frontier.put(next, priority)
                came_from[next] = current

    return came_from, cost_so_far

def reconstruct_path(came_from, start, goal):
    current = goal
    path = [current]
    while current != start:
        current = came_from[current]
        path.append(current)
    path.reverse()  # optional
    return path

def get_neighbors(self, position):
    neighbors = []
    for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:  # Directions: up, down, left, right
        new_x, new_y = position[0] + dx, position[1] + dy
        if 0 <= new_x < self.grid_size and 0 <= new_y < self.grid_size:
            neighbors.append((new_x, new_y))
    return neighbors

env = AdvancedRobotEnv()
start = (0, 0)  # Starting position
goal = (env.grid_size - 1, env.grid_size - 1)  # Goal position
came_from, cost_so_far = a_star_search(env, start, goal)
path = reconstruct_path(came_from, start, goal)
print("Path from start to goal:", path)



import time

# Evaluate Q-learning
def evaluate_q_learning(env, agent, start, goal):
    state = start
    total_reward = 0
    path_length = 0
    start_time = time.time()

    while state != goal:
        action = agent.choose_action(str(state))
        state, reward, done, _ = env.step(action)
        total_reward += reward
        path_length += 1
        if done:
            break

    end_time = time.time()
    return path_length, total_reward, end_time - start_time

# Q-learning metrics
q_path_length, q_total_reward, q_computation_time = evaluate_q_learning(env, agent, start, goal)

# Evaluate A* Search
def evaluate_a_star(env, start, goal):
    start_time = time.time()
    _, cost_so_far = a_star_search(env, start, goal)
    end_time = time.time()
    path = reconstruct_path(came_from, start, goal)

    path_length = len(path) - 1  # Subtracting 1 because the start position is included in the path
    path_cost = cost_so_far[goal]
    computation_time = end_time - start_time

    return path_length, path_cost, computation_time

# A* metrics
a_path_length, a_path_cost, a_computation_time = evaluate_a_star(env, start, goal)

print("Q-Learning:")
print(f"Path Length: {q_path_length}, Total Reward: {q_total_reward}, Computation Time: {q_computation_time} seconds")

print("\nA* Search:")
print(f"Path Length: {a_path_length}, Path Cost: {a_path_cost}, Computation Time: {a_computation_time} seconds")

n_runs = 50
q_learning_metrics = {'path_length': [], 'total_reward': [], 'computation_time': []}
a_star_metrics = {'path_length': [], 'path_cost': [], 'computation_time': []}

for _ in range(n_runs):
    # Q-learning metrics
    q_path_length, q_total_reward, q_computation_time = evaluate_q_learning(env, agent, start, goal)
    q_learning_metrics['path_length'].append(q_path_length)
    q_learning_metrics['total_reward'].append(q_total_reward)
    q_learning_metrics['computation_time'].append(q_computation_time)

    # A* metrics
    a_path_length, a_path_cost, a_computation_time = evaluate_a_star(env, start, goal)
    a_star_metrics['path_length'].append(a_path_length)
    a_star_metrics['path_cost'].append(a_path_cost)
    a_star_metrics['computation_time'].append(a_computation_time)

import matplotlib.pyplot as plt

# Function to plot metrics
def plot_metrics(metrics, title, ylabel):
    plt.figure()
    plt.title(title)
    plt.xlabel('Runs')
    plt.ylabel(ylabel)
    plt.plot(metrics['q_learning'], label='Q-Learning')
    plt.plot(metrics['a_star'], label='A* Search')
    plt.legend()
    plt.show()

# Prepare data for plotting
plot_data = {
    'path_length': {'q_learning': q_learning_metrics['path_length'], 'a_star': a_star_metrics['path_length']},
    'total_reward': {'q_learning': q_learning_metrics['total_reward'], 'a_star': a_star_metrics['path_cost']},  # Note: For A* it's path cost
    'computation_time': {'q_learning': q_learning_metrics['computation_time'], 'a_star': a_star_metrics['computation_time']}
}

# Plotting each metric
plot_metrics(plot_data['path_length'], "Path Length Comparison", "Path Length")
plot_metrics(plot_data['total_reward'], "Total Reward / Path Cost Comparison", "Score / Cost")
plot_metrics(plot_data['computation_time'], "Computation Time Comparison", "Time (seconds)")